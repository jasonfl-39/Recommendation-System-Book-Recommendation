# -*- coding: utf-8 -*-
"""Jason Filbert Leo_Recommendation System_Coursera Courses.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B6iKScwl4XtA-kvGt5-A_bhECQChsk6e

# Data Understanding

Mendownload data, dari https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset
"""

!kaggle datasets download -d arashnic/book-recommendation-dataset

"""unzip data"""

import zipfile
zip = '/content/book-recommendation-dataset.zip'
zip_ref = zipfile.ZipFile(zip, 'r')
zip_ref.extractall()
zip_ref.close()

"""Membaca file-file csv yang telah diunzip"""

# loading dataset
import pandas as pd
book_dir = '/content/Books.csv'
user_dir = '/content/Users.csv'
rating_dir = '/content/Ratings.csv'
book_df = pd.read_csv(book_dir)
user_df = pd.read_csv(user_dir)
rating_df = pd.read_csv(rating_dir)

"""# Exploratory Data Analysis

Overview data pada book_df
"""

book_df.head()

"""Overview data pada user_df"""

user_df.head()

"""Overview data pada rating_df"""

rating_df.head()

"""Shape dari book_df, user_df, rating_df"""

print(book_df.shape)
print(user_df.shape)
print(rating_df.shape)

"""mendrop data gambar pada book_df karena tidak diperlukan"""

book_df = book_df.drop(['Image-URL-S', 'Image-URL-M', 'Image-URL-L'], axis=1)

"""Mengubah nama-nama kolom dengan '-' supaya mudah dibaca pada python dan menghitung jumlah entri data unik"""

book_df = book_df.rename(columns ={"Book-Title" : "Book_Title", "Book-Author" : "Book_Author", "Year-Of-Publication" : "Year"})
user_df = user_df.rename(columns ={"User-ID" : "User_ID"})
rating_df = rating_df.rename(columns ={"User-ID" : "User_ID","Book-Rating" : "Book_Rating"})

print('Jumlah entri unik data buku: ', len(book_df.ISBN.unique()))
print('Jumlah entri unik data user: ', len(user_df.User_ID.unique()))

"""Overview jenis data pada book_df, terlihat bahwa ada data null pada kolom Book_Author dan Publisher"""

book_df.info()

"""Mengubah format tahun publikasi ke angka, diperoleh beberapa sampel data pada kolom Year menjadi null"""

book_df.Year = pd.to_numeric(book_df.Year, errors='coerce')
print('Tahun publikasi: ',sorted(book_df['Year'].unique()))
book_df.info()

"""Jumlah ISBN, judul buku, penulis, tahun terbit, dan penerbit unik"""

print('Jumlah ISBN: ', len(book_df.ISBN.unique()))
print('Jumlah Judul Buku: ', len(book_df.Book_Title.unique()))
print('Jumlah Penulis: ', len(book_df.Book_Author.unique()))
print('Jumlah Tahun Terbit: ', len(book_df.Year.unique()))
print('Jumlah Penerbit: ', len(book_df.Publisher.unique()))

"""Mendrop data kosong"""

book_df = book_df.loc[(pd.isna(book_df[['Book_Title','Year','Publisher']])==False).all(axis=1)]
book_df.shape

"""Overview jenis data pada user_df, terdapat data user dengan kolom Age kosong"""

user_df.info()

"""Karena jumlah data kosong pada kolom Age signifikan, akan dimasukkan dengan nilai mean dari usia user"""

user_df.Age = user_df.Age.fillna(user_df.Age.mean())

"""Overview jenis data pada rating_df, tidak ada data null"""

rating_df.info()

"""Jumlah pengguna, buku, dan rating unik pada data rating"""

print('Jumlah userID: ', len(rating_df.User_ID.unique()))
print('Jumlah ISBN: ', len(rating_df.ISBN.unique()))
print('Jumlah data rating: ', len(rating_df))

"""Nilai rating berada pada range 1-10 untuk penilaian eksplisit, atau 0 pada penilaian implisit"""

rating_explicit = rating_df[rating_df['Book_Rating'] != 0]
rating_implicit = rating_df[rating_df['Book_Rating'] == 0]
print('Jumlah rating eksplisit: ',rating_explicit.shape)
print('Jumlah rating implisit: ',rating_implicit.shape)

"""Nilai statistika pada rating eksplisit"""

rating_explicit.describe()

"""User yang memberikan rating eksplisit dan yang memberikan rating implisit"""

user_explicit = user_df[user_df['User_ID'].isin(rating_explicit['User_ID'])]
user_implicit = user_df[user_df['User_ID'].isin(rating_implicit['User_ID'])]
print('Jumlah user rating eksplisit: ',user_explicit.shape)
print('Jumlah user rating implisit: ',user_implicit.shape)

"""Buku yang dirating eksplisit dan dirating implisit"""

book_explicit = book_df[book_df['ISBN'].isin(rating_explicit['ISBN'])]
book_implicit = book_df[book_df['ISBN'].isin(rating_implicit['ISBN'])]
print('Jumlah rating buku eksplisit: ',book_explicit.shape)
print('Jumlah rating buku implisit: ',book_implicit.shape)

"""# Data Preprocessing

Penggabungan data user
"""

import numpy as np
user_all = np.concatenate((user_df.User_ID.unique(),rating_df.User_ID.unique()))
user_all = np.sort(np.unique(user_all))
print('Jumlah seluruh data user berdasarkan User_ID: ', len(user_all))

"""Penggabungan data buku"""

book_all = np.concatenate((book_df.ISBN.unique(),rating_df.ISBN.unique()))
book_all = np.sort(np.unique(book_all))
print('Jumlah seluruh data buku berdasarkan ISBN: ', len(book_all))

"""Jumlah rating pada data"""

books = pd.merge(rating_df, book_df , on='ISBN', how='left')
books

"""Mengecek data null pada rating buku"""

books.isnull().sum()

"""Jumlah rating buku berdasarkan ISBN"""

books.groupby('ISBN').sum()

"""Menggabungkan dengan fitur Book_Title"""

all_books_rate = rating_df
all_books_title = pd.merge(all_books_rate, book_explicit[['ISBN','Book_Title']], on='ISBN', how='left')
all_books_title

"""Penggabungan data eksplisit atau untuk rating 1-10 saja"""

user_exp = np.concatenate((user_explicit.User_ID.unique(),rating_explicit.User_ID.unique()))
user_exp = np.sort(np.unique(user_exp))
print('Jumlah data user eksplisit berdasarkan User_ID: ', len(user_exp))

book_exp = np.concatenate((book_explicit.ISBN.unique(),rating_explicit.ISBN.unique()))
book_exp = np.sort(np.unique(book_exp))
print('Jumlah data buku eksplisit berdasarkan ISBN: ', len(book_exp))

"""Mengecek data null pada rating buku eksplisit"""

books_exp = pd.merge(rating_explicit, book_explicit , on='ISBN', how='left')
books_exp.isnull().sum()

"""Menggabungkan dengan fitur Book_Title"""

exp_books_rate = rating_explicit
exp_books_title = pd.merge(exp_books_rate, book_explicit[['ISBN','Book_Title']], on='ISBN', how='left')
exp_books_title

"""# Data Preparation

Mengecek missing value pada all_books_title
"""

all_books_title.isnull().sum()

"""Mendrop missing value"""

all_books_clean = all_books_title.dropna()
all_books_clean.isnull().sum()

"""Sorting seluruh data buku menurut ISBN"""

book_sort = all_books_clean.sort_values('ISBN', ascending=True)
book_sort

"""Jumlah ISBN unik pada data"""

len(book_sort.ISBN.unique())

"""Mendrop nilai duplikat"""

book_sort = book_sort.drop_duplicates('ISBN')
book_sort

"""Konversi ISBN dan judul buku ke list"""

isbn = book_sort['ISBN'].tolist()
title = book_sort['Book_Title'].tolist()

print(len(isbn))
print(len(title))

"""Mengubah data menjadi dictionary"""

book_dict = pd.DataFrame({
    'isbn': isbn,
    'title': title,
})
book_dict

"""Mengecek missing value pada exp_books_title"""

exp_books_title.isnull().sum()

"""Mendrop missing value"""

exp_books_clean = exp_books_title.dropna()
exp_books_clean.isnull().sum()

"""Hanya rating eksplisit digunakan untuk training dan validasi data

Membuat ISBN pada rating eksplisit menjadi list dan melakukan encoding pada ISBN
"""

isbns = exp_books_clean['ISBN'].unique().tolist()
print('list ISBN: ', isbns)

book_isbn_encoded = {x: i for i, x in enumerate(isbns)}
print('encoded ISBN : ', book_isbn_encoded)

book_encoded_to_isbn = {i: x for i, x in enumerate(isbns)}
print('encoded angka ke ISBN: ', book_encoded_to_isbn)

"""Pemetaan encoding ke dataframe"""

exp_books_clean['book'] = exp_books_clean['ISBN'].map(book_isbn_encoded)

"""Jumlah buku dan mengubah nilai rating ke float dari rating eksplisit"""

num_user = len(exp_books_clean['User_ID'])
num_book = len(book_encoded_to_isbn)
exp_books_clean['Book_Rating'] = exp_books_clean['Book_Rating'].values.astype(np.float32)
min_rating = min(exp_books_clean['Book_Rating'])
max_rating = max(exp_books_clean['Book_Rating'])

print(' Number of Users: {}, Number of Book: {}, Min Rating: {}, Max Rating: {}'.format(
    num_user, num_book, min_rating, max_rating
))

"""Pengacakan data sebelum dibagi ke Training set dan Validation set"""

df = exp_books_clean.sample(frac=1, random_state=42)
df

"""Pembagian train dan validasi, pada rasio 90:10"""

x = df[['User_ID', 'book']].values
y = df['Book_Rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

train_indices = int(0.9 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x)
print(y)

"""# Model Development

Model Collaborative Filtering
"""

import tensorflow as tf
class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_user, num_book, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_user = num_user
    self.num_book = num_book
    self.embedding_size = embedding_size
    self.user_embedding = tf.keras.layers.Embedding(
        num_user,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = tf.keras.regularizers.l2(1e-6)
    )
    self.user_bias = tf.keras.layers.Embedding(num_user, 1)
    self.book_embedding = tf.keras.layers.Embedding(
        num_book,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = tf.keras.regularizers.l2(1e-6)
    )
    self.book_bias = tf.keras.layers.Embedding(num_book, 1)

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0])
    user_bias = self.user_bias(inputs[:, 0])
    book_vector = self.book_embedding(inputs[:, 1])
    book_bias = self.book_bias(inputs[:, 1])

    dot_user_book = tf.tensordot(user_vector, book_vector, 2)

    x = dot_user_book + user_bias + book_bias

    return tf.nn.sigmoid(x)

"""Compile model"""

model = RecommenderNet(num_user, num_book, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""# Evaluasi model

Fitting model untuk evaluasi model
"""

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 512,
    epochs = 10,
    validation_data = (x_val, y_val)
)

"""Plot error model dengan setiap kali training"""

import matplotlib.pyplot as plt
plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""Cek rekomendasi buku, dengan mengecek juga buku apa saja yang user telah lihat termasuk yang diberi rating nol/implisit dari book_dict. Diambil user secara acak untuk pengecekan."""

book_df = book_dict

user_id = rating_df.User_ID.sample(1).iloc[0]
book_seen = rating_df[rating_df.User_ID == user_id]

book_not_seen = book_df[~book_df['isbn'].isin(book_seen.ISBN.values)]['isbn']
book_not_seen = list(set(book_not_seen).intersection(set(book_isbn_encoded.keys())))

book_not_seen = [[book_isbn_encoded.get(x)] for x in book_not_seen]
user_book_array = np.hstack(
    ([[user_id]] * len(book_not_seen), book_not_seen)
)

"""mendapatkan rekomendasi 10 buku terbaik untuk user berdasarkan buku yang telah dirating user"""

ratings = model.predict(user_book_array).flatten()
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_book_ids = [
    book_encoded_to_isbn.get(book_not_seen[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Book with high ratings from user')
print('----' * 8)

top_book_user = (
    book_seen.sort_values(
        by = 'Book_Rating',
        ascending=False
    )
    .head(5)
    .ISBN.values
)

book_df_rows = book_df[book_df['isbn'].isin(top_book_user)]
for row in book_df_rows.itertuples():
    print(row.isbn, ':', row.title)

print('----' * 8)
print('Top 10 book recommendation')
print('----' * 8)

recommended_book = book_df[book_df['isbn'].isin(recommended_book_ids)]
for row in recommended_book.itertuples():
    print(row.isbn, ':', row.title)